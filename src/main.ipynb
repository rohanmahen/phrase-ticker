{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# phrase-ticker Dataset Creation\n",
    "\n",
    "This notebook demonstrates how to create a dataset linking S&P 500 company tickers with natural language phrases, using web scraping for data collection and GPT-4 for phrase generation. Designed for easy integration with the Hugging Face `datasets` library, this tool aims to enhance the extraction of stock tickers from textual data, offering valuable resources for financial NLP tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import libraries\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# import environment variables\n",
    "import dotenv\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting S&P 500 Company Information\n",
    "\n",
    "In this section, we retrieve a list of S&P 500 companies and their stock ticker symbols directly from Wikipedia. Using Python's `requests` library, we access the webpage containing the S&P 500 index composition. With `BeautifulSoup`, we parse the HTML content to locate and extract the data from the relevant table. Each row of the table provides the ticker symbol and company name, which we collect into a structured list for further processing. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([{'ticker': 'MMM', 'name': '3M'},\n",
       "  {'ticker': 'AOS', 'name': 'A. O. Smith'},\n",
       "  {'ticker': 'ABT', 'name': 'Abbott'},\n",
       "  {'ticker': 'ABBV', 'name': 'AbbVie'},\n",
       "  {'ticker': 'ACN', 'name': 'Accenture'},\n",
       "  {'ticker': 'ADBE', 'name': 'Adobe Inc.'},\n",
       "  {'ticker': 'AMD', 'name': 'Advanced Micro Devices'},\n",
       "  {'ticker': 'AES', 'name': 'AES Corporation'},\n",
       "  {'ticker': 'AFL', 'name': 'Aflac'},\n",
       "  {'ticker': 'A', 'name': 'Agilent Technologies'}],\n",
       " 503)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the S&P 500 companies from Wikipedia\n",
    "url = 'https://en.wikipedia.org/wiki/List_of_S%26P_500_companies'\n",
    "\n",
    "# Get the page\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the page\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Get the table\n",
    "table = soup.find('table', {'id': 'constituents'})\n",
    "\n",
    "# Get the rows\n",
    "rows = table.find_all('tr')[1:]  # Skip the header row\n",
    "\n",
    "# Get the compnay name and ticket\n",
    "data = []\n",
    "for row in rows:\n",
    "    cols = row.find_all('td')\n",
    "    symbol = cols[0].text.strip()\n",
    "    name = cols[1].text.strip() \n",
    "    data.append({'ticker': symbol, 'name': name})\n",
    "\n",
    "data[:10], len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MMM</td>\n",
       "      <td>3M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AOS</td>\n",
       "      <td>A. O. Smith</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ABT</td>\n",
       "      <td>Abbott</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ABBV</td>\n",
       "      <td>AbbVie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ACN</td>\n",
       "      <td>Accenture</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ticker         name\n",
       "0    MMM           3M\n",
       "1    AOS  A. O. Smith\n",
       "2    ABT       Abbott\n",
       "3   ABBV       AbbVie\n",
       "4    ACN    Accenture"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create dataframe\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phrase Generation with GPT-4\n",
    "\n",
    "This section uses the GPT-4 model to generate 50 unique phrases for each S&P 500 company, enhancing our dataset for ticker extraction tasks. The process, facilitated by LangChain and OpenAI APIs, dynamically creates phrases reflecting each company's attributes, making the data more versatile for NLP applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'companyName': '3M',\n",
       "  'ticker': 'MMM',\n",
       "  'phrases': ['Innovative materials giant',\n",
       "   'Post-it Notes creator',\n",
       "   'Scotch tape manufacturer',\n",
       "   'Multinational conglomerate powerhouse',\n",
       "   'Diversified technology company',\n",
       "   'Leader in consumer goods',\n",
       "   'Healthcare product innovator',\n",
       "   'Safety and industrial products supplier',\n",
       "   'Famous for its adhesive solutions',\n",
       "   'Global science company',\n",
       "   'Makers of N95 respirators',\n",
       "   'Renowned for its research and development',\n",
       "   'Enterprise supplying office supplies',\n",
       "   'Pioneer in home improvement products',\n",
       "   'Automotive industry supplier',\n",
       "   'Electronics and energy products manufacturer',\n",
       "   'Provider of filtration solutions',\n",
       "   'Expert in manufacturing abrasives',\n",
       "   'Key player in the construction market',\n",
       "   'Inventor of reflective road signs',\n",
       "   'Specialist in dental and orthodontic products',\n",
       "   'Industry leader in adhesives and tapes',\n",
       "   'Supplier of personal protective equipment',\n",
       "   'Innovation-driven conglomerate',\n",
       "   'Globally recognized brand in consumer products',\n",
       "   'Major player in the healthcare sector',\n",
       "   'Trusted source of industrial adhesives',\n",
       "   'Creator of numerous patented technologies',\n",
       "   'Sustainability-focused company',\n",
       "   'Veteran in the manufacturing sector',\n",
       "   'Prominent in the science and innovation field',\n",
       "   'Essential provider of workspace solutions',\n",
       "   'Champion of worker safety products',\n",
       "   'Committed to science for a better life',\n",
       "   'Influential in the automotive and aerospace industries',\n",
       "   'Leader in window films and coatings',\n",
       "   'Renowned for its command strips and hooks',\n",
       "   'A driving force in the electronics industry',\n",
       "   'Provider of comprehensive healthcare solutions',\n",
       "   'Integral in advancements in digital health',\n",
       "   'Developer of groundbreaking medical devices',\n",
       "   'Front-runner in environmental health and safety',\n",
       "   'Manufacturer of high-performance materials',\n",
       "   'Key innovator in the graphic design sector',\n",
       "   'Trusted partner in transportation solutions',\n",
       "   'Expert in noise-reduction technologies',\n",
       "   'Leading the way in energy management',\n",
       "   'Pioneer in fire protection products',\n",
       "   'Respected name in the commercial solutions industry',\n",
       "   'Home to a wide range of consumer brands']},\n",
       " dict)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "import dotenv\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "output_parser = JsonOutputParser()\n",
    "\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "   \n",
    "    You are now Phrase Generator, your job is to take in a company name, and ticker and generate natural language phrases\n",
    "    that would be useful for people looking for that company\n",
    "\n",
    "    the company name is: {name}\n",
    "    the company ticker is: {ticker}\n",
    "\n",
    "\n",
    "    For example if the company name is Apple, the description is iPhone maker, and the ticker is AAPL,\n",
    "    you might generate phrases like \"Big tech company\" or \"steve jobs company\" etc\n",
    "\n",
    "    To generate phrases for the company use your training data and the supplied description.\n",
    "\n",
    "    Generate 50 casual natural language phrases.\n",
    "     {format_instructions}\n",
    "\n",
    "    \"\"\",\n",
    "    input_variables=[\"name\", \"ticker\"],\n",
    "    partial_variables={\"format_instructions\": output_parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4-0125-preview\", api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "chain = prompt | model | output_parser\n",
    "\n",
    "phrases = chain.invoke({\"name\": df[\"name\"][0], \"ticker\": df[\"ticker\"][0]})\n",
    "phrases, type(phrases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Concurrent Phrase Generation\n",
    "\n",
    "This section accelerates phrase generation by using `concurrent.futures` for parallel processing. It simultaneously invokes the GPT-4 model for each S&P 500 company, updating our dataset with generated phrases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import concurrent.futures\n",
    "import json\n",
    "\n",
    "def generate_phrases(index, name, ticker):\n",
    "    try:\n",
    "        response = chain.invoke({\"name\": name, \"ticker\": ticker})\n",
    "        phrases = response.get(\"phrases\", [])\n",
    "        print(f\"Successfully processed {ticker} - {name}\")\n",
    "        return index, json.dumps(phrases)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {ticker} - {name}: {e}\")\n",
    "        return index, json.dumps([])\n",
    "\n",
    "def update_phrases(df, results):\n",
    "    for index, phrases in results.items():\n",
    "        df.at[index, \"phrases\"] = phrases\n",
    "    print(\"Update complete.\")\n",
    "\n",
    "def main(df):\n",
    "    num_workers = 30\n",
    "    results = {}\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
    "        future_to_row = {executor.submit(generate_phrases, i, row['name'], row['ticker']): i for i, row in df.iterrows()}\n",
    "        for future in concurrent.futures.as_completed(future_to_row):\n",
    "            index, phrases = future.result()\n",
    "            results[index] = phrases\n",
    "            print(f\"Row {index} updated with phrases.\")\n",
    "    \n",
    "    update_phrases(df, results)\n",
    "\n",
    "# Call main function to process the DataFrame\n",
    "main(df)\n",
    "\n",
    "# Check the updated DataFrame\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filling Missing Phrases\n",
    "\n",
    "This step identifies and fills missing phrases for using the GPT-4 model, ensuring our dataset is complete before saving it to a new CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if any, find the companies where phrases is []\n",
    "df[df['phrases'] == '[]']\n",
    "\n",
    "# i got the following companies with phrases []\n",
    "# \tticker\tname\tphrases\n",
    "# 284\tLRCX\tLam Research\t[]\n",
    "# 488\tWDC\tWestern Digital\t[]\n",
    "\n",
    "#  add back the phrases for the companies where phrases is []\n",
    "for index, row in df.iterrows():\n",
    "    if row['phrases'] == '[]':\n",
    "        print(f\"Processing {row['ticker']} - {row['name']}\")\n",
    "        response = chain.invoke({\"name\": row['name'], \"ticker\": row['ticker']})\n",
    "        phrases = response.get(\"phrases\", [])\n",
    "        df.at[index, \"phrases\"] = json.dumps(phrases)\n",
    "\n",
    "# df to new csv\n",
    "df.to_csv('../data/raw.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pivoting Dataset for phrase-ticker Mapping\n",
    "\n",
    "This section transforms the original dataset into a new format where each row represents a unique combination of a phrase and its corresponding ticker symbol. By iterating over each company's phrases, we create a more granular dataset suitable for NLP tasks focused on ticker extraction. The final structured dataset is saved to CSV, facilitating easy access and further use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "\n",
    "# Initialize a list to store the new rows (phrase, ticker)\n",
    "new_data = []\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    # Convert the string representation of the list back to a list\n",
    "    phrases_list = ast.literal_eval(row['phrases'])\n",
    "    \n",
    "    # For each phrase, append a new row to new_data\n",
    "    for phrase in phrases_list:\n",
    "        new_data.append({'phrase': phrase, 'ticker': row['ticker']})\n",
    "\n",
    "# Create a new DataFrame from the new_data list\n",
    "new_df = pd.DataFrame(new_data, columns=['phrase', 'ticker'])\n",
    "new_df.head()\n",
    "\n",
    "# save to csv optionally\n",
    "new_df.to_csv('../data/data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['phrase', 'ticker'],\n",
      "    num_rows: 25181\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# Create a Hugging Face dataset from the new DataFrame\n",
    "dataset = Dataset.from_pandas(new_df)\n",
    "\n",
    "# Display the first few rows to verify\n",
    "print(dataset)\n",
    "\n",
    "# discrepancy in num rows could be because gpt occasionally did 51 phrases instead of 50"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
